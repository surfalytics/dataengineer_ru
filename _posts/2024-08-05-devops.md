---
title: DevOps
author: Дата Инженеръ
date: 2024-08-05
category: hard-skills
layout: post
section: Инструменты
cover: ../assets/surf2.svg
---

**DevOps**-принципы и практики широко применяются в области Data Engineering и Data Analytics, обеспечивая более эффективное управление данными, автоматизацию и масштабируемость. 

DevOps и Data Engineering/Data Analytics неразрывно связаны. Эта связь обеспечивает более эффективное
управление данными, автоматизацию и масштабируемость. Есть даже специальный термин: **DataOps** — это эволюция DevOps 
для обработки данных (по классике DevOps, включает в себя аспекты разработки, автоматизации и аналитики).

## DevOps и Data Engineering: Общие принципы

Data Engineering интегрируется с DevOps в следующих ключевых аспектах:

1. **Автоматизация и CI/CD**  
   Использование практик CI/CD позволяет автоматизировать процессы загрузки данных, их трансформации и доставки 
юзерам. Это упрощает разработку data pipelines.

2. **Инфраструктура как код (IaC)**  
   Kubernetes, Terraform и Ansible позволяют управлять инфраструктурой, которая работает с данными, в виде кода. Это 
повышает воспроизводимость, снижает человеческие ошибки и упрощает масштабирование.

3. **Мониторинг и логирование**  
   Prometheus, Grafana и ELK, помогают отслеживать производительность ETL процессов, выявлять сбои и обеспечивать 
стабильность систем аналитики (при правильной настройке monitoring/alerting).

## Что такое DataOps?

[DataOps](https://ru.wikipedia.org/wiki/DataOps) — это подход, основанный на принципах DevOps, но адаптированный для 
обработки данных. Его цель — ускорение разработки data pipelines, повышение качества данных и упрощение анализа. 
Основные аспекты DataOps включают:

1. **Гибкость и адаптивность**  
   CI/CD и IaC помогают обрабатывать данные быстрее и адаптироваться к изменениям (например, изменяя конфигурационные 
файлы) и ускорять разработку новых pipeline.

2. **Качество данных и тестирование**  
   Автоматическое тестирование данных гарантирует, что изменения в pipeline не влияют на их качество. Это включает 
проверки данных на корректность, отсутствие пропусков и согласованность.

3. **Улучшенная коллаборация**  
   DataOps объединяет аналитиков, инженеров данных и DevOps-специалистов, помгая взаимодействовать разным командам и
совместно использовать инструменты.

## Git и версионность 

[Git](https://git-scm.com/) — это распределённая система контроля версий, которая стала стандартом в современной разработке, включая Data Engineering. В контексте Data Engineering и Analytics, Git и системы версионного контроля применяются в следующих ключевых аспектах:

1. **Версионирование кода и конфигураций**  
   Git позволяет отслеживать изменения в коде ETL-процессов, SQL-запросах и конфигурационных файлах. Это обеспечивает 
возможность отката к предыдущим версиям и помогает понимать, кто и когда внёс изменения в data pipeline.

2. **Командная работа и code review**  
   Через механизмы pull/merge requests команды могут эффективно проводить code review изменений в ETL-процессах, 
проверять качество кода и обмениваться знаниями. Это особенно важно при работе с критически важными data pipelines.

3. **Интеграция с CI/CD**  
   Современные Git-платформы предоставляют встроенные инструменты для CI/CD (например, GitHub Actions, GitLab CI/CD), 
что позволяет автоматизировать тестирование, развертывание и валидацию данных при каждом изменении в репозитории.

Основные платформы для хостинга Git-репозиториев:

- [GitHub](https://github.com/) — крупнейшая платформа для хостинга открытого кода с широкими возможностями для CI/CD и командной работы
- [GitLab](https://gitlab.com/) — наиболее популярная платформа в концепции self-hosted, предоставляющая полный набор инструментов DevOps
- [Bitbucket](https://bitbucket.org/) — платформа от Atlassian, тесно интегрированная с другими продуктами компании
- [Gitea](https://gitea.io/) — легковесная self-hosted платформа, простая в установке и администрировании

## Repositoty Templates

Repository Templates (шаблоны репозиториев) — это предопределённые структуры проектов, которые помогают стандартизировать разработку и упростить начало новых проектов. В контексте Data Engineering они особенно важны для обеспечения согласованности структуры проектов и следования лучшим практикам.

- [Cookiecutter](https://cookiecutter-data-science.drivendata.org/) — инструмент для создания шаблонов репозиториев. Устанавливается как Python библиотека и позволяет создавать репозитории с предопределённой структурой файлов и директорий.
- [Kedro](https://kedro.org/) — фреймворк для создания data pipelines, который включает в себя шаблоны репозиториев для разработки ETL-процессов. Позволяет создавать проекты с предопределённой структурой и инструментами для разработки.

## Инструменты для работы с зависимостями

Управление зависимостями критически важно для воспроизводимости data engineering проектов, etl-pipelines. Современные инструменты предоставляют разные подходы к решению этой задачи.

- [Pyenv](https://github.com/pyenv/pyenv) — инструмент для управления версиями Python. Позволяет устанавливать и использовать разные версии Python на одной машине.
- [Poetry](https://python-poetry.org/) — инструмент для управления зависимостями в Python проектах. Позволяет создавать виртуальные окружения, устанавливать и обновлять зависимости, а также управлять версиями пакетов.
- [Conda](https://docs.conda.io/en/latest/) — пакетный менеджер и среда управления зависимостями для Python и других языков программирования. Позволяет создавать виртуальные окружения, устанавливать пакеты и управлять их версиями.
- [Pipenv](https://pipenv.pypa.io/en/latest/) — инструмент для управления зависимостями в Python проектах. С помощью Pipenv можно создавать виртуальные окружения, устанавливать и обновлять зависимости, и управлять версиями пакетов.

## Formatters

Форматтеры кода — это инструменты, которые автоматически приводят код к единому стилю, обеспечивая его читаемость и соответствие принятым стандартам. В контексте Data Engineering они особенно важны для поддержания качества кода в data pipelines и аналитических скриптах.

- [Black](https://black.readthedocs.io/) — популярный форматтер для Python, который применяет строгие правила форматирования. Это помогает избежать споров о стиле кода в команде и обеспечивает единообразие.
- [isort](https://pycqa.github.io/isort/) — инструмент для автоматической сортировки и форматирования импортов в Python файлах. Помогает организовать импорты в логические группы и поддерживает их в порядке. Сортировка происходит согласно PEP 8.
- Также полезно использовать специальный файл `.editorconfig`, в корне репозитория, для установки общих правил форматирования кода в проекте. Это позволяет установить правила форматирования для разных типов файлов (например, Python, SQL, YAML) и поддерживать их в актуальном состоянии.

Зачем нужны форматтеры:

- Автоматизация рутинной задачи по форматированию кода
- Устранение споров в команде о стиле кодирования
- Повышение читаемости кода для всех участников проекта
- Экономия времени при code review, так как вам уже не нужно обсуждать стиль
- Единообразие кода во всём проекте независимо от автора изменений

## Linters

Линтеры (статические анализаторы кода) - анализируют код на наличие потенциальных ошибок, проблем с качеством и отклонений от принятых стандартов кодирования. В Data Engineering они помогают обеспечить надёжность и поддерживаемость data pipelines.

1. **Основные линтеры для Python**
   - [Pylint](https://pylint.org/) — комплексный линтер, который проверяет соответствие стандартам PEP 8, выявляет ошибки и предлагает рекомендации по улучшению кода
   - [Flake8](https://flake8.pycqa.org/) — комбинация нескольких инструментов для проверки стиля и качества кода
   - [Ruff](https://github.com/astral-sh/ruff) — быстрый линтер написанный на Rust, поддерживающий большинство проверок Flake8
   - [mypy](https://mypy.readthedocs.io/) — статический анализатор типов для Python, который помогает выявлять ошибки типизации

2. **Специализированные линтеры для SQL**
   - [SQLFluff](https://www.sqlfluff.com/) — линтер для SQL, который помогает поддерживать единый стиль SQL-запросов в проекте
   - [sql-lint](https://github.com/joereynolds/sql-lint) — простой линтер для проверки базового синтаксиса SQL
  
Зачем нужны линтеры:

- Раннее/автоматическое обнаружение потенциальных ошибок в коде
- Обеспечение соответствия кода стандартам и лучшим практикам
- Поддержание качества кода на протяжении всего жизненного цикла проекта
- Упрощение процесса code review за счёт автоматических проверок
- Обучение разработчиков лучшим практикам через подсказки линтера

Классический набор питониста: `black`, `isort`, `flake8`, `mypy`. 

## Тестирование

Тестирование в Data Engineering охватывает не только проверку логики кода, но и валидацию данных, проверку ETL-процессов и тестирование производительности pipeline. Основные аспекты тестирования включают:

1. **Unit-тестирование (Модульное тестирование)**
   
   Мы берём маленькую часть кода (например, одну функцию) и проверяем, правильно ли она работает сама по себе. Допустим, у вас есть функция, которая очищает данные от пропусков — вы проверяете, действительно ли она удаляет пропуски и не портит при этом остальные данные.

   Популярные инструменты:
   - [pytest](https://docs.pytest.org/) — основной фреймворк для написания и выполнения тестов в Python
   - [unittest](https://docs.python.org/3/library/unittest.html) — встроенный в стандартную библиотеку Python модуль для создания unit-тестов
   - [nose2](https://docs.nose2.io/) — расширенная версия unittest с дополнительными возможностями

2. **Тестирование данных (Data Testing)**
   
   Пример тестирования данных. Берем нужные данные и проверяем, что в колонке с возрастом нет отрицательных значений, что почтовые адреса соответствуют правильному формату, или что общая сумма продаж не превышает реалистичных значений.

   Основные инструменты:
   - [Great Expectations](https://greatexpectations.io/) — фреймворк для валидации, документирования и профилирования данных
   - [Pandas Testing](https://pandas.pydata.org/docs/reference/testing.html) — модуль для тестирования операций с DataFrame

3. **Интеграционное тестирование (Integration Testing)**
   
   Это как проверка работы всего механизма в сборе. Если unit-тесты проверяют отдельные шестерёнки, то интеграционные тесты проверяют, как эти шестерёнки работают вместе. Например, мы проверяем, правильно ли данные:
   - Загружаются из базы данных
   - Проходят все этапы обработки
   - Сохраняются в конечное хранилище
   
   При этом мы тестируем взаимодействие с реальными системами или их имитациями.

   Основные инструменты:
   - [TestContainers](https://testcontainers-python.readthedocs.io/) — библиотека для тестирования с использованием Docker контейнеров (например, можно поднять временную базу данных для тестов)
   - [Moto](https://github.com/getmoto/moto) — библиотека для мокирования AWS сервисов в тестах (позволяет имитировать работу с S3, DynamoDB и другими сервисами AWS)

Зачем нужно тестирование:

- Гарантия корректной работы ETL-процессов и трансформаций данных
- Обеспечение надёжности data pipeline при изменениях в коде
- Проверка качества и целостности данных на всех этапах обработки данных
- Выявление проблем производительности до релиза в продакшн
- Документирование ожидаемого поведения системы через тесты
- Упрощение рефакторинга и доработки существующего кода проектов

## Docker и Docker Compose

## Kubernetes и Minikube

## CI/CD

## Semantic Versioning

## Инфраструктура как код

## Мониторинг и логирование
